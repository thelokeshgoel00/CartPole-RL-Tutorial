
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "# Reinforcement Learning with Keras + OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "## What is Reinforcement Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "- It is the training of **ML models** to make a **sequence of decisions**\n",
        "- The method employs **Hit and Trial** to get solution\n",
        "- **Rewards** are given by games\n",
        "- Goal is to **Maximize** the **Total Rewards**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "<img src=\"reinforcepic.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "## What is Keras?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "Keras is one of the leading high-level neural networks APIs. It is written in Python and supports multiple back-end neural network computation engines.\n",
        "- with Keras we do not need to make backpropogation algorithms\n",
        "- Many Layers could be added in just few lines of code\n",
        "- All the types of models are built on same principles hence it becomes easier to master"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "<img src = \"neuralnetworkgif.gif\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "## What is OpenAI Gym?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "- Gym is a toolkit for developing and comparing reinforcement learning algorithms.\n",
        "- It supports teaching agents everything from walking to playing games like Pong or Pinball.\n",
        "- Has easy implementation in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "## For Starters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "- You need to install OpenAI gym package available at https://gym.openai.com\n",
        "- You can run pip install gym on terminal\n",
        "or !pip install gym on jupyter notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "## Learning On CartPole\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "<img src = \"cartpole.gif\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "- A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
        "The system is controlled by applying a force of +1 or -1 to the cart.\n",
        "- The pendulum starts upright, and the goal is to prevent it from falling over.\n",
        "- A reward of +1 is provided for every timestep that the pole remains upright.\n",
        "- The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
        "- Read more about it on https://gym.openai.com/envs/CartPole-v0/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "## Understanding OpenAI Gym Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "- OpenAI Gym environments are structured around two main parts: an observation space and an action space\n",
        "- Based On The current state of observation, we determine the action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "### Using Gym\n",
        "- ##### create environment of gym\n",
        "env = gym.make('env-name')\n",
        "- #####  reset the environment\n",
        "env.reset()\n",
        "- #####  render the environment onto visible game\n",
        "env.render()\n",
        "- ##### Take next step in game\n",
        "env.step('give your action')\n",
        "- ##### close the rendering window\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        
      },
      "outputs": [
        
      ],
      "source": [
        "import gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        
      },
      "outputs": [
        
      ],
      "source": [
        "env = gym.make('CartPole-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        
      },
      "outputs": [
        
      ],
      "source": [
        "observation = env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ]
        }
      ],
      "source": [
        "t=0\n",
        "while(t<1000):\n",
        "    env.render()\n",
        "    env.step()\n",
        "    t+=1\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        
      },
      "outputs": [
        
      ],
      "source": [
        
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "# Data Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "- We will collect data by running a certain number of Random trials\n",
        "- Only those trials will be considered that have got us a min score\n",
        "- One Hot Encoding will be used for passing action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        
      },
      "outputs": [
        
      ],
      "source": [
        "def gather_data(env):\n",
        "    num_trials = 10000\n",
        "    min_score = 50\n",
        "    sim_steps = 300\n",
        "    trainingX,trainingY = [],[]\n",
        "    scores = []\n",
        "    for trial in range(num_trials):\n",
        "        observation = env.reset()\n",
        "        score = 0\n",
        "        training_sampleX,training_sampleY = [],[]\n",
        "        for step in range(sim_steps):\n",
        "            if(trial%400==0):\n",
        "                env.render()\n",
        "            action = np.random.randint(0,2) # left or right\n",
        "            one_hot_action = np.zeros(2)\n",
        "            one_hot_action[action] = 1\n",
        "            training_sampleX.append(observation)\n",
        "            training_sampleY.append(one_hot_action)\n",
        "            observation , reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "        if score>min_score:\n",
        "            scores.append(score)\n",
        "            trainingX+=training_sampleX\n",
        "            trainingY+=training_sampleY\n",
        "    trainingX,trainingY = np.array(trainingX), np.array(trainingY)\n",
        "    print(\"Average: {}\".format(np.mean(scores)))\n",
        "    print(\"Median: {}\".format(np.median(scores)))\n",
        "    return trainingX,trainingY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average: 62.154798761609904\n",
            "Median: 59.0\n",
            "(array([[-0.02925428,  0.02547628,  0.01115619, -0.0022209 ],\n",
            "       [-0.02874476,  0.22043648,  0.01111177, -0.29136314],\n",
            "       [-0.02433603,  0.41539824,  0.00528451, -0.58052094],\n",
            "       ...,\n",
            "       [ 0.20535798, -0.65948285,  0.13012252,  1.63853338],\n",
            "       [ 0.19216832, -0.46610441,  0.16289319,  1.38906612],\n",
            "       [ 0.18284623, -0.27334315,  0.19067451,  1.15143091]]), array([[0., 1.],\n",
            "       [0., 1.],\n",
            "       [0., 1.],\n",
            "       ...,\n",
            "       [0., 1.],\n",
            "       [0., 1.],\n",
            "       [1., 0.]]))\n"
          ]
        }
      ],
      "source": [
        "print(gather_data(env))\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "# Model Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "- We will use keras for model definition\n",
        "- The model we use here is a very simple one: several fully-connected layers\n",
        "- We can use enhancement such as Convolutions, LSTM,Dropouts etc.\n",
        "- Input will be the observation and output will be action\n",
        "- Loss can be used are mean_squared_error, categorical_crossentropy etc.\n",
        "- Preferred optimizer is usually adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        
      },
      "outputs": [
        
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        
      },
      "outputs": [
        
      ],
      "source": [
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128,input_shape=(4,),activation='relu'))\n",
        "    model.add(Dropout(0.6))\n",
        "\n",
        "    model.add(Dense(256,activation='relu'))\n",
        "    model.add(Dropout(0.6))\n",
        "\n",
        "    model.add(Dense(512,activation='relu'))\n",
        "    model.add(Dropout(0.6))\n",
        "\n",
        "    model.add(Dense(256,activation='relu'))\n",
        "    model.add(Dropout(0.6))\n",
        "\n",
        "    model.add(Dense(128,activation='relu'))\n",
        "    model.add(Dropout(0.6))\n",
        "\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    model.summary()\n",
        "\n",
        "    model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        
      },
      "source": [
        "- From the data gathered above we train our data\n",
        "- We will go through several trials to check on multiple cases\n",
        "- In each trial score we get a score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        
      },
      "outputs": [
        
      ],
      "source": [
        "def predict():\n",
        "    env1 = gym.make('CartPole-v0')\n",
        "    trainingX,trainingY = gather_data(env1)\n",
        "    model = create_model()\n",
        "    model.fit(trainingX,trainingY,epochs=5)\n",
        "    \n",
        "    scores = []\n",
        "    num_trials = 50\n",
        "    sim_steps = 300\n",
        "    for trial in range(num_trials):\n",
        "        observation = env1.reset()\n",
        "        score = 0\n",
        "        for step in range(sim_steps):\n",
        "            if(trial%4==0):\n",
        "                env1.render()\n",
        "            action = np.argmax(model.predict(observation.reshape(1,4)))\n",
        "            observation,reward,done,info = env1.step(action)\n",
        "            \n",
        "            if done:\n",
        "                score+=reward\n",
        "                break\n",
        "        scores.append(score)\n",
        "        print(np.mean(scores))\n",
        "    env1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average: 63.26243093922652\n",
            "Median: 60.0\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 128)               640       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 2)                 258       \n",
            "=================================================================\n",
            "Total params: 329,730\n",
            "Trainable params: 329,730\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 22901 samples\n",
            "Epoch 1/5\n",
            "22901/22901 [==============================] - 5s 222us/sample - loss: 0.2482 - accuracy: 0.5609\n",
            "Epoch 2/5\n",
            "22901/22901 [==============================] - 3s 141us/sample - loss: 0.2432 - accuracy: 0.5839\n",
            "Epoch 3/5\n",
            "22901/22901 [==============================] - 3s 145us/sample - loss: 0.2417 - accuracy: 0.5891\n",
            "Epoch 4/5\n",
            "22901/22901 [==============================] - 3s 142us/sample - loss: 0.2406 - accuracy: 0.5978\n",
            "Epoch 5/5\n",
            "22901/22901 [==============================] - 4s 153us/sample - loss: 0.2407 - accuracy: 0.5882\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        
      },
      "outputs": [
        
      ],
      "source": [
        
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
